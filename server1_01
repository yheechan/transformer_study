using data from torchtext.legacy
{   'batch_size': 8,
    'dropout': 0.0,
    'hidden_size': 16,
    'lang': 'enko',
    'lr': 0.0001,
    'max_length': 50,
    'n_epochs': 20,
    'n_layers': 4,
    'n_splits': 8,
    'train': 'corpus.shuf.train.tok.bpe',
    'use_adam': False,
    'use_transformer': True,
    'valid': 'corpus.shuf.valid.tok.bpe'}

input_size:  58365
output_size:  140364

 Transformer(
  (emb_enc): Embedding(58365, 16)
  (emb_dec): Embedding(140364, 16)
  (emb_dropout): Dropout(p=0.0, inplace=False)
  (encoder): MySequential(
    (0): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
    (1): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
    (2): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
    (3): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (decoder): MySequential(
    (0): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.0, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
    (1): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.0, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
    (2): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.0, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
    (3): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.0, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.0, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.0, inplace=False)
    )
  )
  (generator): Sequential(
    (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=16, out_features=140364, bias=True)
    (2): LogSoftmax(dim=-1)
  )
)

 Loss function: Negative Log-Likelihood with log-probability (NLLLoss)

Using device number: 0
Optimizer: Adam
Start training...
 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |  10.122601   |  8.971254  | 8.067906 | 17.31  | 94.46 
   2    |   8.445697   | 14.019575  | 7.342934 | 23.08  | 93.84 
   3    |   7.823959   | 18.555762  | 6.954605 | 25.00  | 94.65 
   4    |   7.453015   | 22.324799  | 6.689433 | 30.77  | 94.50 
   5    |   7.201704   | 25.169305  | 6.393174 | 33.65  | 94.53 
   6    |   7.018374   | 27.307443  | 6.206322 | 35.58  | 94.51 
   7    |   6.871010   | 29.091031  | 6.052908 | 38.46  | 94.43 
   8    |   6.741409   | 30.480830  | 5.930188 | 39.42  | 94.64 
   9    |   6.632132   | 31.637142  | 5.814986 | 39.42  | 94.60 
  10    |   6.540549   | 32.528475  | 5.719538 | 39.42  | 94.58 
  11    |   6.468401   | 33.262287  | 5.689293 | 40.38  | 94.15 
  12    |   6.418742   | 33.653048  | 5.615169 | 42.31  | 94.67 
  13    |   6.381530   | 34.061968  | 5.623903 | 43.27  | 94.67 
  14    |   6.346965   | 34.270118  | 5.639681 | 43.27  | 94.37 
  15    |   6.329161   | 34.458728  | 5.656318 | 43.27  | 94.43 
  16    |   6.333001   | 34.468471  | 5.657649 | 43.27  | 94.73 
  17    |   6.370092   | 34.249210  | 5.690131 | 43.27  | 94.61 
  18    |   6.444413   | 34.229947  | 5.795336 | 41.35  | 94.46 
  19    |   6.518710   | 34.276738  | 5.889892 | 41.35  | 94.30 
  20    |   6.598126   | 34.484724  | 6.063916 | 42.31  | 94.17 


Training complete! Best accuracy: 43.27%.
total taken time:  31.488426587584158
