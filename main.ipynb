{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using data from torchtext.legacy\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from data_loader import DataLoader\n",
    "import data_loader\n",
    "import trainer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import timeit\n",
    "\n",
    "from models.transformer import Transformer\n",
    "import model_util as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, output_size, \n",
    "    hidden_size=32,\n",
    "    n_splits=8,\n",
    "    n_layers=4,\n",
    "    dropout=0.0,\n",
    "    use_transformer=True):\n",
    "\n",
    "\tif use_transformer:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def get_crit(output_size, pad_index):\n",
    "\t# Default weight for loss equals to 1, but we don't need to get loss for PAD token\n",
    "\t# Thus, set a weight for PAD to zero.\n",
    "\tloss_weight = torch.ones(output_size)\n",
    "\tloss_weight[pad_index] = 0.\n",
    "\n",
    "\t# Instead of using Cross-Entropy Loss,\n",
    "\t# we can use Negative Log-Likelihood(NLL) Loss with log-probability.\n",
    "\tprint('\\n Loss function: Negative Log-Likelihood with log-probability (NLLLoss)')\n",
    "\tcrit = nn.NLLLoss(\n",
    "\t\tweight=loss_weight,\n",
    "\t\treduction='sum',\n",
    "\t)\n",
    "\n",
    "\treturn crit\n",
    "\n",
    "\n",
    "def get_optimizer(model, \n",
    "    use_adam=True,\n",
    "    use_transformer=True,\n",
    "    lr=0.0001,):\n",
    "\tif use_adam:\n",
    "\t\tif use_transformer:\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\telse: # case of rnn based seq2seq\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\telse:\n",
    "\t\tprint('Optimizer: Adam')\n",
    "\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        'corpus.shuf.train.tok.bpe',\n",
    "        'corpus.shuf.valid.tok.bpe',\n",
    "        ('en', 'ko'),                           # Source and target language.\n",
    "        batch_size=32,\n",
    "        device=-1,                              # Lazy loading\n",
    "        max_length=25,                          # Loger sequence will be excluded.\n",
    "        dsl=False,                              # Turn-off Dual-supervised Learning mode.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_size:  24088\n",
      "output_size:  43711\n"
     ]
    }
   ],
   "source": [
    "input_size, output_size = len(loader.src.vocab), len(loader.tgt.vocab)\n",
    "print('\\ninput_size: ', input_size)\n",
    "print('output_size: ', output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Transformer(\n",
      "  (emb_enc): Embedding(24088, 64)\n",
      "  (emb_dec): Embedding(43711, 64)\n",
      "  (emb_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (encoder): MySequential(\n",
      "    (0): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): MySequential(\n",
      "    (0): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (2): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (3): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (K_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (V_linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (linear): Linear(in_features=64, out_features=64, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=64, out_features=256, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=256, out_features=64, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=64, out_features=43711, bias=True)\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = get_model(input_size, output_size,\n",
    "    hidden_size=128,\n",
    "    n_splits=8,\n",
    "    n_layers=4,\n",
    "    dropout=0.0,\n",
    "    use_transformer=True)\n",
    "print('\\n', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loss function: Negative Log-Likelihood with log-probability (NLLLoss)\n"
     ]
    }
   ],
   "source": [
    "crit = get_crit(output_size, data_loader.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device number: 0\n"
     ]
    }
   ],
   "source": [
    "# if model_weight is not None:\n",
    "    # model.load_state_dict(model_weight)\n",
    "\n",
    "# check for available gpu\n",
    "if torch.cuda.is_available():\n",
    "    device_num = 0\n",
    "    print('\\nUsing device number: 0')\n",
    "else:\n",
    "    device_num = -1\n",
    "    print('\\nUsing device number: -1')\n",
    "\n",
    "# Clear memory cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Pass model to GPU device if it is necessary\n",
    "if device_num >= 0:\n",
    "    model.cuda(device_num)\n",
    "    crit.cuda(device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, lr=0.003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if opt_weight is not None and config.use_adam:\n",
    "    # optimizer.load_state_dict(opt_weight)\n",
    "\n",
    "lr_schedular = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_title = 'local1'\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('./tensorboard/'+overall_title+'/tests')\n",
    "\n",
    "title = overall_title + '_02'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      " Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed\n",
      "--------------------------------------------------------------------------------\n",
      "   1    |   6.044504   | 39.004591  | 3.652802 | 46.25  | 13.09 \n",
      "   2    |   3.637714   | 59.758172  | 3.117372 | 52.50  | 12.95 \n",
      "   3    |   3.190355   | 62.954866  | 3.247136 | 52.08  | 13.26 \n",
      "   4    |   3.097017   | 63.014154  | 3.522850 | 50.42  | 13.13 \n",
      "   5    |   3.020030   | 62.703817  | 3.542941 | 51.25  | 13.04 \n",
      "   6    |   2.980572   | 62.695145  | 3.468065 | 55.00  | 13.03 \n",
      "   7    |   2.926913   | 62.994757  | 3.554330 | 53.33  | 12.73 \n",
      "   8    |   2.826470   | 63.440163  | 3.932808 | 53.33  | 13.06 \n",
      "   9    |   2.742568   | 63.735462  | 4.061507 | 52.50  | 13.04 \n",
      "  10    |   2.635356   | 64.642559  | 3.624754 | 54.17  | 12.94 \n",
      "  11    |   2.552841   | 65.033947  | 3.171433 | 52.50  | 12.93 \n",
      "  12    |   2.475214   | 65.624768  | 3.644714 | 51.67  | 12.97 \n",
      "  13    |   2.391760   | 66.151868  | 3.488067 | 55.42  | 13.05 \n",
      "  14    |   2.329351   | 66.978899  | 3.518302 | 54.17  | 13.00 \n",
      "  15    |   2.273245   | 67.363981  | 3.452144 | 52.50  | 12.88 \n",
      "  16    |   2.224594   | 67.744750  | 3.689554 | 53.33  | 12.60 \n",
      "  17    |   2.162065   | 68.170161  | 3.652301 | 52.92  | 13.00 \n",
      "  18    |   2.166684   | 68.084185  | 3.930809 | 50.00  | 13.06 \n",
      "  19    |   2.133199   | 68.642966  | 3.859914 | 52.50  | 12.93 \n",
      "  20    |   2.123720   | 68.457195  | 3.931179 | 52.50  | 13.13 \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 55.42%.\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "trainer.train(\n",
    "    model,\n",
    "    crit,\n",
    "    optimizer,\n",
    "    train_loader=loader.train_iter,\n",
    "    valid_loader=loader.valid_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    n_epochs=20,\n",
    "    lr_schedular=lr_schedular,\n",
    "    writer=writer,\n",
    "    title=title,\n",
    ")\n",
    "\n",
    "end_time = (timeit.default_timer() - start_time) / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.329987438216661"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mu.saveModel(overall_title, title, model)\n",
    "# mu.graphModel(train_dataloader, model, writer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = mu.getModel(overall_title, title)\n",
    "# print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
