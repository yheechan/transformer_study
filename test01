using data from torchtext.legacy
{   'batch_size': 8,
    'dropout': 0.2,
    'hidden_size': 16,
    'lang': 'enko',
    'lr': 0.0001,
    'max_length': 50,
    'n_epochs': 20,
    'n_layers': 4,
    'n_splits': 8,
    'train': 'corpus.shuf.train.tok.bpe',
    'use_adam': False,
    'use_transformer': True,
    'valid': 'corpus.shuf.valid.tok.bpe'}

input_size:  58365
output_size:  140364

 Transformer(
  (emb_enc): Embedding(58365, 16)
  (emb_dec): Embedding(140364, 16)
  (emb_dropout): Dropout(p=0.2, inplace=False)
  (encoder): MySequential(
    (0): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
    (1): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
    (2): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
    (3): EncoderBlock(
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (decoder): MySequential(
    (0): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.2, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
    (1): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.2, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
    (2): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.2, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
    (3): DecoderBlock(
      (masked_attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (masked_attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (masked_attn_dropout): Dropout(p=0.2, inplace=False)
      (attn): MultiHead(
        (Q_linear): Linear(in_features=16, out_features=16, bias=False)
        (K_linear): Linear(in_features=16, out_features=16, bias=False)
        (V_linear): Linear(in_features=16, out_features=16, bias=False)
        (linear): Linear(in_features=16, out_features=16, bias=False)
        (attn): Attention(
          (softmax): Softmax(dim=-1)
        )
      )
      (attn_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (attn_dropout): Dropout(p=0.2, inplace=False)
      (fc): Sequential(
        (0): Linear(in_features=16, out_features=64, bias=True)
        (1): ReLU()
        (2): Linear(in_features=64, out_features=16, bias=True)
      )
      (fc_norm): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
      (fc_dropout): Dropout(p=0.2, inplace=False)
    )
  )
  (generator): Sequential(
    (0): LayerNorm((16,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=16, out_features=140364, bias=True)
    (2): LogSoftmax(dim=-1)
  )
)

 Loss function: Negative Log-Likelihood with log-probability (NLLLoss)

Using device number: 0
Optimizer: Adam
Start training...
 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |  10.227296   |  7.860977  | 8.438547 | 17.31  | 96.17 
   2    |   8.875778   | 11.753725  | 7.864338 | 23.08  | 96.43 
   3    |   8.435600   | 14.291599  | 7.426956 | 25.00  | 96.22 
   4    |   8.127422   | 16.503358  | 7.088263 | 27.88  | 96.21 
   5    |   7.899048   | 18.303430  | 6.803052 | 29.81  | 95.88 
   6    |   7.729039   | 19.950733  | 6.580136 | 32.69  | 96.21 
   7    |   7.598346   | 21.532626  | 6.443671 | 33.65  | 96.19 
   8    |   7.497735   | 22.728354  | 6.367034 | 34.62  | 96.42 
   9    |   7.412351   | 23.716171  | 6.292991 | 34.62  | 96.40 
  10    |   7.341058   | 24.596397  | 6.220086 | 34.62  | 95.54 
  11    |   7.287449   | 25.182114  | 6.155494 | 34.62  | 96.31 
  12    |   7.236429   | 25.737486  | 6.143259 | 34.62  | 96.71 
  13    |   7.195780   | 26.055399  | 6.108833 | 35.58  | 96.16 
  14    |   7.156127   | 26.413137  | 6.067632 | 35.58  | 95.85 
  15    |   7.122684   | 26.687147  | 6.035697 | 35.58  | 96.22 
  16    |   7.088974   | 26.906149  | 6.015006 | 38.46  | 96.17 
  17    |   7.066985   | 27.060987  | 5.971964 | 39.42  | 96.21 
  18    |   7.055297   | 27.148014  | 5.958377 | 39.42  | 96.13 
  19    |   7.059068   | 27.072370  | 5.983725 | 38.46  | 96.38 
  20    |   7.078279   | 26.992760  | 6.032954 | 37.50  | 96.55 


Training complete! Best accuracy: 39.42%.
