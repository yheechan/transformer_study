{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using data from torchtext.legacy\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from data_loader import DataLoader\n",
    "import data_loader\n",
    "import trainer\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import timeit\n",
    "\n",
    "from models.transformer import Transformer\n",
    "import model_util as mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, output_size, \n",
    "    hidden_size=32,\n",
    "    n_splits=8,\n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    use_transformer=True):\n",
    "\n",
    "\tif use_transformer:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def get_crit(output_size, pad_index):\n",
    "\t# Default weight for loss equals to 1, but we don't need to get loss for PAD token\n",
    "\t# Thus, set a weight for PAD to zero.\n",
    "\tloss_weight = torch.ones(output_size)\n",
    "\tloss_weight[pad_index] = 0.0\n",
    "\n",
    "\t# Instead of using Cross-Entropy Loss,\n",
    "\t# we can use Negative Log-Likelihood(NLL) Loss with log-probability.\n",
    "\tprint('\\n Loss function: Negative Log-Likelihood with log-probability (NLLLoss)')\n",
    "\tcrit = nn.NLLLoss(\n",
    "\t\tweight=loss_weight,\n",
    "\t\treduction='sum',\n",
    "\t)\n",
    "\n",
    "\treturn crit\n",
    "\n",
    "\n",
    "def get_optimizer(model, \n",
    "    use_adam=True,\n",
    "    use_transformer=True,\n",
    "    lr=0.0001,):\n",
    "\tif use_adam:\n",
    "\t\tif use_transformer:\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\telse: # case of rnn based seq2seq\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\telse:\n",
    "\t\tprint('Optimizer: Adam')\n",
    "\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "dropout = 0.0\n",
    "hidden_size = 64\n",
    "lang = ('en', 'ko')\n",
    "lr = 0.0003\n",
    "max_length = 20\n",
    "n_epochs = 10\n",
    "n_layers = 4\n",
    "n_splits = 8\n",
    "research_num = '01'\n",
    "research_subject = 'merged_update'\n",
    "test_fn = 'corpus.shuf.test.tok.bpe'\n",
    "train_fn = 'corpus.shuf.train.tok.bpe'\n",
    "valid_fn = 'corpus.shuf.valid.tok.bpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        train_fn=train_fn,\n",
    "        valid_fn=valid_fn,\n",
    "        test_fn=test_fn,\n",
    "        exts=lang,\n",
    "        batch_size=batch_size,\n",
    "        device=-1,                                      # Lazy loading\n",
    "        max_length=max_length,                          # Loger sequence will be excluded.\n",
    "        dsl=False,                                      # Turn-off Dual-supervised Learning mode.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_size:  15884\n",
      "output_size:  26204\n"
     ]
    }
   ],
   "source": [
    "input_size, output_size = len(loader.src.vocab), len(loader.tgt.vocab)\n",
    "print('\\ninput_size: ', input_size)\n",
    "print('output_size: ', output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_size, output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    n_splits=n_splits,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    use_transformer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loss function: Negative Log-Likelihood with log-probability (NLLLoss)\n"
     ]
    }
   ],
   "source": [
    "crit = get_crit(output_size, data_loader.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if model_weight is not None:\n",
    "    # model.load_state_dict(model_weight)\n",
    "\n",
    "# check for available gpu\n",
    "if torch.cuda.is_available():\n",
    "    device_num = 0\n",
    "    print('\\nUsing device number: 0')\n",
    "else:\n",
    "    device_num = -1\n",
    "    print('\\nUsing device number: -1')\n",
    "\n",
    "# Clear memory cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Pass model to GPU device if it is necessary\n",
    "if device_num >= 0:\n",
    "    model.cuda(device_num)\n",
    "    crit.cuda(device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if opt_weight is not None and config.use_adam:\n",
    "    # optimizer.load_state_dict(opt_weight)\n",
    "\n",
    "lr_schedular = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_title = 'local1'\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('./tensorboard/'+overall_title+'/tests')\n",
    "\n",
    "title = overall_title + '_08'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "trainer.train(\n",
    "    model,\n",
    "    crit,\n",
    "    optimizer,\n",
    "    train_loader=loader.train_iter,\n",
    "    valid_loader=loader.valid_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    n_epochs=20,\n",
    "    lr_schedular=lr_schedular,\n",
    "    writer=writer,\n",
    "    title=title,\n",
    ")\n",
    "\n",
    "end_time = (timeit.default_timer() - start_time) / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.saveModel(overall_title, title, model)\n",
    "# mu.graphModel(train_dataloader, model, writer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (emb_enc): Embedding(15884, 256)\n",
      "  (emb_dec): Embedding(26204, 256)\n",
      "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (encoder): MySequential(\n",
      "    (0): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (3): EncoderBlock(\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (decoder): MySequential(\n",
      "    (0): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (1): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (2): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "    (3): DecoderBlock(\n",
      "      (masked_attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (masked_attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (masked_attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (attn): MultiHead(\n",
      "        (Q_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (K_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (V_linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (linear): Linear(in_features=256, out_features=256, bias=False)\n",
      "        (attn): Attention(\n",
      "          (softmax): Softmax(dim=-1)\n",
      "        )\n",
      "      )\n",
      "      (attn_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn_dropout): Dropout(p=0.2, inplace=False)\n",
      "      (fc): Sequential(\n",
      "        (0): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "      )\n",
      "      (fc_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc_dropout): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (1): Linear(in_features=256, out_features=26204, bias=True)\n",
      "    (2): LogSoftmax(dim=-1)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = mu.getModel(overall_title, title)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device number: 0\n"
     ]
    }
   ],
   "source": [
    "loss, acc = tester.test(\n",
    "    model,\n",
    "    crit,\n",
    "    test_loader=loader.test_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    lr_schedular=lr_schedular,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.914247512817383\n",
      "47.109375\n"
     ]
    }
   ],
   "source": [
    "print(loss)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
