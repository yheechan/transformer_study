{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using data from torchtext.legacy\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from data_loader import DataLoader\n",
    "import data_loader\n",
    "import trainer\n",
    "import tester\n",
    "from models.transformer import Transformer\n",
    "import model_util as mu\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, output_size, \n",
    "    hidden_size=32,\n",
    "    n_splits=8,\n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    use_transformer=True):\n",
    "\n",
    "\tif use_transformer:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def get_crit(output_size, pad_index):\n",
    "\t# Default weight for loss equals to 1, but we don't need to get loss for PAD token\n",
    "\t# Thus, set a weight for PAD to zero.\n",
    "\tloss_weight = torch.ones(output_size)\n",
    "\tloss_weight[pad_index] = 0.0\n",
    "\n",
    "\t# Instead of using Cross-Entropy Loss,\n",
    "\t# we can use Negative Log-Likelihood(NLL) Loss with log-probability.\n",
    "\tprint('\\n Loss function: Negative Log-Likelihood with log-probability (NLLLoss)')\n",
    "\tcrit = nn.NLLLoss(\n",
    "\t\tweight=loss_weight,\n",
    "\t\treduction='sum',\n",
    "\t)\n",
    "\n",
    "\treturn crit\n",
    "\n",
    "\n",
    "def get_optimizer(model, \n",
    "    use_adam=True,\n",
    "    use_transformer=True,\n",
    "    lr=0.0001,):\n",
    "\tif use_adam:\n",
    "\t\tif use_transformer:\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\telse: # case of rnn based seq2seq\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\telse:\n",
    "\t\tprint('Optimizer: Adam')\n",
    "\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dropout = 0.0\n",
    "hidden_size = 128\n",
    "lang = ('en', 'ko')\n",
    "lr = 0.0003\n",
    "max_length = 20\n",
    "n_epochs = 30\n",
    "n_layers = 4\n",
    "n_splits = 8\n",
    "research_num = '01'\n",
    "research_subject = 'local_medium1'\n",
    "test_fn = 'corpus.shuf.test.tok.bpe'\n",
    "train_fn = 'corpus.shuf.train.tok.bpe'\n",
    "valid_fn = 'corpus.shuf.valid.tok.bpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        train_fn=train_fn,\n",
    "        valid_fn=valid_fn,\n",
    "        test_fn=test_fn,\n",
    "        exts=lang,\n",
    "        batch_size=batch_size,\n",
    "        device=-1,                                      # Lazy loading\n",
    "        max_length=max_length,                          # Loger sequence will be excluded.\n",
    "        dsl=False,                                      # Turn-off Dual-supervised Learning mode.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_size:  69459\n",
      "output_size:  154233\n"
     ]
    }
   ],
   "source": [
    "input_size, output_size = len(loader.src.vocab), len(loader.tgt.vocab)\n",
    "print('\\ninput_size: ', input_size)\n",
    "print('output_size: ', output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_size, output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    n_splits=n_splits,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    use_transformer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loss function: Negative Log-Likelihood with log-probability (NLLLoss)\n"
     ]
    }
   ],
   "source": [
    "crit = get_crit(output_size, data_loader.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device number: 0\n"
     ]
    }
   ],
   "source": [
    "# if model_weight is not None:\n",
    "    # model.load_state_dict(model_weight)\n",
    "\n",
    "# check for available gpu\n",
    "if torch.cuda.is_available():\n",
    "    device_num = 0\n",
    "    print('\\nUsing device number: 0')\n",
    "else:\n",
    "    device_num = -1\n",
    "    print('\\nUsing device number: -1')\n",
    "\n",
    "# Clear memory cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Pass model to GPU device if it is necessary\n",
    "if device_num >= 0:\n",
    "    model.cuda(device_num)\n",
    "    crit.cuda(device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if opt_weight is not None and config.use_adam:\n",
    "    # optimizer.load_state_dict(opt_weight)\n",
    "\n",
    "lr_schedular = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_title = research_subject\n",
    "title = subject_title + '_' + research_num\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('../tensorboard/'+subject_title+'/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      " Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed\n",
      "--------------------------------------------------------------------------------\n",
      "   1    |   6.644376   | 39.431375  | 4.332067 | 35.08  | 53.05 \n",
      "   2    |   3.876657   | 61.164154  | 3.201270 | 40.47  | 52.07 \n",
      "   3    |   3.048784   | 68.533197  | 2.686681 | 42.97  | 52.75 \n",
      "   4    |   2.629587   | 72.256897  | 2.451242 | 43.91  | 52.82 \n",
      "   5    |   2.396824   | 74.402395  | 2.387501 | 45.16  | 54.95 \n",
      "   6    |   2.282867   | 75.562616  | 2.362514 | 45.47  | 53.80 \n",
      "   7    |   2.241073   | 76.227784  | 2.311289 | 45.39  | 52.68 \n",
      "   8    |   2.228202   | 76.647878  | 2.289201 | 45.39  | 53.93 \n",
      "   9    |   2.231472   | 76.853735  | 2.296090 | 45.55  | 52.55 \n",
      "  10    |   2.242944   | 76.913748  | 2.308893 | 45.55  | 52.86 \n",
      "  11    |   2.259382   | 76.888497  | 2.327871 | 45.62  | 52.28 \n",
      "  12    |   2.277931   | 76.818067  | 2.345445 | 45.47  | 52.65 \n",
      "  13    |   2.297559   | 76.727367  | 2.366982 | 45.47  | 52.91 \n",
      "  14    |   2.317305   | 76.624778  | 2.384420 | 45.47  | 53.41 \n",
      "  15    |   2.336394   | 76.531474  | 2.409251 | 45.39  | 53.81 \n",
      "  16    |   2.355056   | 76.429904  | 2.424788 | 45.39  | 52.66 \n",
      "  17    |   2.372124   | 76.320521  | 2.451163 | 45.39  | 52.41 \n",
      "  18    |   2.388652   | 76.201853  | 2.462407 | 45.39  | 52.32 \n",
      "  19    |   2.403466   | 76.105568  | 2.488538 | 45.39  | 53.41 \n",
      "  20    |   2.417834   | 75.989504  | 2.497414 | 45.39  | 53.02 \n",
      "  21    |   2.430749   | 75.884160  | 2.522708 | 45.39  | 52.24 \n",
      "  22    |   2.443080   | 75.769946  | 2.528497 | 45.47  | 52.64 \n",
      "  23    |   2.454453   | 75.671018  | 2.552160 | 45.47  | 53.64 \n",
      "  24    |   2.465159   | 75.552727  | 2.556448 | 45.39  | 52.40 \n",
      "  25    |   2.475167   | 75.461537  | 2.582962 | 45.31  | 53.00 \n",
      "  26    |   2.484017   | 75.342907  | 2.591423 | 45.31  | 52.62 \n",
      "  27    |   2.491968   | 75.255378  | 2.609648 | 45.39  | 53.53 \n",
      "  28    |   2.497630   | 75.146297  | 2.613368 | 45.39  | 52.28 \n",
      "  29    |   2.504488   | 75.065335  | 2.631262 | 45.23  | 52.26 \n",
      "  30    |   2.513422   | 74.955273  | 2.633153 | 45.23  | 53.89 \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 45.62%.\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "trainer.train(\n",
    "    model,\n",
    "    crit,\n",
    "    optimizer,\n",
    "    train_loader=loader.train_iter,\n",
    "    valid_loader=loader.valid_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    n_epochs=n_epochs,\n",
    "    lr_schedular=lr_schedular,\n",
    "    writer=writer,\n",
    "    title=title,\n",
    ")\n",
    "\n",
    "end_time = (timeit.default_timer() - start_time) / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time taken:  26.480890219683335\n"
     ]
    }
   ],
   "source": [
    "print('training time taken: ', end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.saveModel(subject_title, title, model)\n",
    "# mu.graphModel(train_dataloader, model, writer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mu.getModel(subject_title, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device number: 0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = tester.test(\n",
    "    model,\n",
    "    crit,\n",
    "    test_loader=loader.test_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    lr_schedular=lr_schedular,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  2.876065254211426\n",
      "test_acc:  47.61904761904762\n"
     ]
    }
   ],
   "source": [
    "print('test loss: ', test_loss)\n",
    "print('test_acc: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
