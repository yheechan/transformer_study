{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using data from torchtext.legacy\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import pprint\n",
    "\n",
    "import torch, gc\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "from data_loader import DataLoader\n",
    "import data_loader\n",
    "import trainer\n",
    "import tester\n",
    "from models.transformer import Transformer\n",
    "import model_util as mu\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(input_size, output_size, \n",
    "    hidden_size=32,\n",
    "    n_splits=8,\n",
    "    n_layers=4,\n",
    "    dropout=0.2,\n",
    "    use_transformer=True):\n",
    "\n",
    "\tif use_transformer:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\telse:\n",
    "\t\tmodel = Transformer(\n",
    "\t\t\tinput_size,\t\t\t\t\t\t# Source vocabulary size\n",
    "\t\t\thidden_size,\t\t\t\t# Transformer doesn't need word_vec_size,\n",
    "\t\t\toutput_size,\t\t\t\t\t# Target vocabulary size\n",
    "\t\t\tn_splits=n_splits,\t\t# Number of head in Multi-head Attention\n",
    "\t\t\tn_enc_blocks=n_layers,\t# number of encoder blocks\n",
    "\t\t\tn_dec_blocks=n_layers,\t# Number of decoder blocks\n",
    "\t\t\tdropout_p=dropout,\t\t# Dropout rate on each block\n",
    "\t\t)\n",
    "\t\n",
    "\treturn model\n",
    "\n",
    "\n",
    "def get_crit(output_size, pad_index):\n",
    "\t# Default weight for loss equals to 1, but we don't need to get loss for PAD token\n",
    "\t# Thus, set a weight for PAD to zero.\n",
    "\tloss_weight = torch.ones(output_size)\n",
    "\tloss_weight[pad_index] = 0.0\n",
    "\n",
    "\t# Instead of using Cross-Entropy Loss,\n",
    "\t# we can use Negative Log-Likelihood(NLL) Loss with log-probability.\n",
    "\tprint('\\n Loss function: Negative Log-Likelihood with log-probability (NLLLoss)')\n",
    "\tcrit = nn.NLLLoss(\n",
    "\t\tweight=loss_weight,\n",
    "\t\treduction='sum',\n",
    "\t)\n",
    "\n",
    "\treturn crit\n",
    "\n",
    "\n",
    "def get_optimizer(model, \n",
    "    use_adam=True,\n",
    "    use_transformer=True,\n",
    "    lr=0.0001,):\n",
    "\tif use_adam:\n",
    "\t\tif use_transformer:\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\telse: # case of rnn based seq2seq\n",
    "\t\t\toptimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\telse:\n",
    "\t\tprint('Optimizer: Adam')\n",
    "\t\toptimizer = optim.Adam(model.parameters(), lr=lr, betas=(.9, .98))\n",
    "\t\n",
    "\treturn optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "dropout = 0.0\n",
    "hidden_size = 128\n",
    "lang = ('en', 'ko')\n",
    "lr = 0.0003\n",
    "max_length = 20\n",
    "n_epochs = 50\n",
    "n_layers = 4\n",
    "n_splits = 8\n",
    "research_num = '01'\n",
    "research_subject = 'local_medium1'\n",
    "test_fn = 'corpus.shuf.test.tok.bpe'\n",
    "train_fn = 'corpus.shuf.train.tok.bpe'\n",
    "valid_fn = 'corpus.shuf.valid.tok.bpe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(\n",
    "        train_fn=train_fn,\n",
    "        valid_fn=valid_fn,\n",
    "        test_fn=test_fn,\n",
    "        exts=lang,\n",
    "        batch_size=batch_size,\n",
    "        device=-1,                                      # Lazy loading\n",
    "        max_length=max_length,                          # Loger sequence will be excluded.\n",
    "        dsl=False,                                      # Turn-off Dual-supervised Learning mode.\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "input_size:  69459\n",
      "output_size:  154233\n"
     ]
    }
   ],
   "source": [
    "input_size, output_size = len(loader.src.vocab), len(loader.tgt.vocab)\n",
    "print('\\ninput_size: ', input_size)\n",
    "print('output_size: ', output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(input_size, output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    n_splits=n_splits,\n",
    "    n_layers=n_layers,\n",
    "    dropout=dropout,\n",
    "    use_transformer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Loss function: Negative Log-Likelihood with log-probability (NLLLoss)\n"
     ]
    }
   ],
   "source": [
    "crit = get_crit(output_size, data_loader.PAD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device number: 0\n"
     ]
    }
   ],
   "source": [
    "# if model_weight is not None:\n",
    "    # model.load_state_dict(model_weight)\n",
    "\n",
    "# check for available gpu\n",
    "if torch.cuda.is_available():\n",
    "    device_num = 0\n",
    "    print('\\nUsing device number: 0')\n",
    "else:\n",
    "    device_num = -1\n",
    "    print('\\nUsing device number: -1')\n",
    "\n",
    "# Clear memory cache\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Pass model to GPU device if it is necessary\n",
    "if device_num >= 0:\n",
    "    model.cuda(device_num)\n",
    "    crit.cuda(device_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = get_optimizer(model, lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if opt_weight is not None and config.use_adam:\n",
    "    # optimizer.load_state_dict(opt_weight)\n",
    "\n",
    "lr_schedular = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject_title = research_subject\n",
    "title = subject_title + '_' + research_num\n",
    "\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('../tensorboard/'+subject_title+'/tests')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      " Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed\n",
      "--------------------------------------------------------------------------------\n",
      "   1    |   6.657451   | 39.054159  | 4.462898 | 34.38  | 53.11 \n",
      "   2    |   3.889955   | 61.082702  | 3.308266 | 40.39  | 53.34 \n",
      "   3    |   3.057791   | 68.476278  | 2.771558 | 43.05  | 53.33 \n",
      "   4    |   2.636499   | 72.185069  | 2.464527 | 44.22  | 52.74 \n",
      "   5    |   2.400443   | 74.336833  | 2.337725 | 45.08  | 52.88 \n",
      "   6    |   2.283756   | 75.533704  | 2.341477 | 45.47  | 53.42 \n",
      "   7    |   2.241941   | 76.201400  | 2.309805 | 45.47  | 54.02 \n",
      "   8    |   2.229728   | 76.609265  | 2.298989 | 45.47  | 53.71 \n",
      "   9    |   2.233934   | 76.819539  | 2.306580 | 45.47  | 52.62 \n",
      "  10    |   2.246141   | 76.878759  | 2.319358 | 45.55  | 53.42 \n",
      "  11    |   2.263491   | 76.851923  | 2.335628 | 45.55  | 53.69 \n",
      "  12    |   2.282192   | 76.785795  | 2.355156 | 45.55  | 54.42 \n",
      "  13    |   2.302610   | 76.696454  | 2.375609 | 45.47  | 53.94 \n",
      "  14    |   2.322296   | 76.594998  | 2.396393 | 45.47  | 54.54 \n",
      "  15    |   2.342201   | 76.491503  | 2.417739 | 45.39  | 56.78 \n",
      "  16    |   2.360337   | 76.386083  | 2.439383 | 45.39  | 54.17 \n",
      "  17    |   2.378396   | 76.282249  | 2.458404 | 45.39  | 53.22 \n",
      "  18    |   2.394214   | 76.171847  | 2.475361 | 45.39  | 54.30 \n",
      "  19    |   2.410000   | 76.060765  | 2.492065 | 45.39  | 53.87 \n",
      "  20    |   2.423495   | 75.943305  | 2.508183 | 45.39  | 51.67 \n",
      "  21    |   2.437610   | 75.837546  | 2.524380 | 45.39  | 53.01 \n",
      "  22    |   2.449178   | 75.713065  | 2.540539 | 45.39  | 52.18 \n",
      "  23    |   2.461970   | 75.609193  | 2.554667 | 45.39  | 52.02 \n",
      "  24    |   2.471601   | 75.489770  | 2.570259 | 45.47  | 51.62 \n",
      "  25    |   2.482739   | 75.393635  | 2.580665 | 45.47  | 52.24 \n",
      "  26    |   2.490840   | 75.272551  | 2.593304 | 45.47  | 52.41 \n",
      "  27    |   2.500661   | 75.185475  | 2.605616 | 45.47  | 52.80 \n",
      "  28    |   2.507669   | 75.075677  | 2.618190 | 45.47  | 52.71 \n",
      "  29    |   2.515937   | 74.998830  | 2.626267 | 45.39  | 53.92 \n",
      "  30    |   2.519057   | 74.919529  | 2.640231 | 45.39  | 53.39 \n",
      "  31    |   2.529274   | 74.839398  | 2.648890 | 45.39  | 52.04 \n",
      "  32    |   2.534902   | 74.780668  | 2.656707 | 45.39  | 52.49 \n",
      "  33    |   2.541555   | 74.722768  | 2.666394 | 45.31  | 52.25 \n",
      "  34    |   2.544238   | 74.686043  | 2.675229 | 45.31  | 52.81 \n",
      "  35    |   2.551958   | 74.629502  | 2.679936 | 45.23  | 53.17 \n",
      "  36    |   2.556196   | 74.590588  | 2.694530 | 45.31  | 52.86 \n",
      "  37    |   2.561314   | 74.553523  | 2.710077 | 45.31  | 52.68 \n",
      "  38    |   2.563247   | 74.517175  | 2.715874 | 45.23  | 52.11 \n",
      "  39    |   2.565908   | 74.492604  | 2.724637 | 45.23  | 52.54 \n",
      "  40    |   2.569762   | 74.460332  | 2.717973 | 45.23  | 52.98 \n",
      "  41    |   2.572419   | 74.439082  | 2.734527 | 45.23  | 51.64 \n",
      "  42    |   2.574092   | 74.404961  | 2.736460 | 45.23  | 52.04 \n",
      "  43    |   2.575227   | 74.373332  | 2.734282 | 45.23  | 51.73 \n",
      "  44    |   2.576312   | 74.346118  | 2.741103 | 45.23  | 52.57 \n",
      "  45    |   2.574913   | 74.327888  | 2.745489 | 45.23  | 52.71 \n",
      "  46    |   2.571331   | 74.309053  | 2.742522 | 45.23  | 52.36 \n",
      "  47    |   2.565502   | 74.284331  | 2.757762 | 45.16  | 51.96 \n",
      "  48    |   2.549333   | 74.281840  | 2.755768 | 45.08  | 51.32 \n",
      "  49    |   2.511926   | 74.378427  | 2.772215 | 45.08  | 52.07 \n",
      "  50    |   2.473658   | 74.477053  | 2.784732 | 45.23  | 53.50 \n",
      "\n",
      "\n",
      "Training complete! Best accuracy: 45.55%.\n"
     ]
    }
   ],
   "source": [
    "start_time = timeit.default_timer()\n",
    "\n",
    "trainer.train(\n",
    "    model,\n",
    "    crit,\n",
    "    optimizer,\n",
    "    train_loader=loader.train_iter,\n",
    "    valid_loader=loader.valid_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    n_epochs=n_epochs,\n",
    "    lr_schedular=lr_schedular,\n",
    "    writer=writer,\n",
    "    title=title,\n",
    ")\n",
    "\n",
    "end_time = (timeit.default_timer() - start_time) / 60.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training time taken:  44.12219217093334\n"
     ]
    }
   ],
   "source": [
    "print('training time taken: ', end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu.saveModel(subject_title, title, model)\n",
    "# mu.graphModel(train_dataloader, model, writer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = mu.getModel(subject_title, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using device number: 0\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = tester.test(\n",
    "    model,\n",
    "    crit,\n",
    "    test_loader=loader.test_iter,\n",
    "    src_vocab=loader.src.vocab,\n",
    "    tgt_vocab=loader.tgt.vocab,\n",
    "    lr_schedular=lr_schedular,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test loss:  3.044724225997925\n",
      "test_acc:  47.17261904761905\n"
     ]
    }
   ],
   "source": [
    "print('test loss: ', test_loss)\n",
    "print('test_acc: ', test_acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
