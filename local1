 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |   9.617900   | 12.011155  | 7.634293 | 21.15  | 67.47 
   2    |   7.863696   | 19.446490  | 6.785726 | 29.81  | 67.09 
   3    |   7.111475   | 26.133362  | 6.223035 | 36.54  | 66.73 
   4    |   6.648754   | 30.799083  | 5.793836 | 41.35  | 67.91 
   5    |   6.339591   | 34.205952  | 5.538150 | 42.31  | 67.30 
   6    |   6.114581   | 36.906260  | 5.396571 | 43.27  | 66.95 
   7    |   5.941995   | 39.180565  | 5.274637 | 43.27  | 68.26 
   8    |   5.796360   | 40.840858  | 5.194319 | 45.19  | 67.34 
   9    |   5.676739   | 42.261472  | 5.112420 | 45.19  | 67.72 
  10    |   5.577651   | 43.439605  | 5.120562 | 46.15  | 67.68 
  11    |   5.500908   | 44.108987  | 5.080782 | 46.15  | 67.67 
  12    |   5.449133   | 44.708882  | 5.095075 | 46.15  | 67.83 
  13    |   5.414469   | 45.120913  | 5.122634 | 46.15  | 67.01 
  14    |   5.390165   | 45.372662  | 5.128598 | 47.12  | 66.64 
  15    |   5.372634   | 45.518926  | 5.164775 | 46.15  | 67.46 
  16    |   5.363818   | 45.636726  | 5.155058 | 46.15  | 66.89 
  17    |   5.365247   | 45.697295  | 5.138142 | 46.15  | 66.82 
  18    |   5.394742   | 45.818688  | 5.242198 | 47.12  | 67.26 
  19    |   5.450750   | 45.795206  | 5.225008 | 47.12  | 67.62 
  20    |   5.534050   | 45.748275  | 5.317689 | 48.08  | 67.56 


Training complete! Best accuracy: 48.08%.

batch_size = 8
dropout = 0.0
hidden_size = 32
lr = 0.0001
max_length = 50
n_epcohs = 20
n_layers = 4
n_splits = 8

input_size:  46332
output_size:  104049

=========================================================================================

 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |   6.044504   | 39.004591  | 3.652802 | 46.25  | 13.09 
   2    |   3.637714   | 59.758172  | 3.117372 | 52.50  | 12.95 
   3    |   3.190355   | 62.954866  | 3.247136 | 52.08  | 13.26 
   4    |   3.097017   | 63.014154  | 3.522850 | 50.42  | 13.13 
   5    |   3.020030   | 62.703817  | 3.542941 | 51.25  | 13.04 
   6    |   2.980572   | 62.695145  | 3.468065 | 55.00  | 13.03 
   7    |   2.926913   | 62.994757  | 3.554330 | 53.33  | 12.73 
   8    |   2.826470   | 63.440163  | 3.932808 | 53.33  | 13.06 
   9    |   2.742568   | 63.735462  | 4.061507 | 52.50  | 13.04 
  10    |   2.635356   | 64.642559  | 3.624754 | 54.17  | 12.94 
  11    |   2.552841   | 65.033947  | 3.171433 | 52.50  | 12.93 
  12    |   2.475214   | 65.624768  | 3.644714 | 51.67  | 12.97 
  13    |   2.391760   | 66.151868  | 3.488067 | 55.42  | 13.05 
  14    |   2.329351   | 66.978899  | 3.518302 | 54.17  | 13.00 
  15    |   2.273245   | 67.363981  | 3.452144 | 52.50  | 12.88 
  16    |   2.224594   | 67.744750  | 3.689554 | 53.33  | 12.60 
  17    |   2.162065   | 68.170161  | 3.652301 | 52.92  | 13.00 
  18    |   2.166684   | 68.084185  | 3.930809 | 50.00  | 13.06 
  19    |   2.133199   | 68.642966  | 3.859914 | 52.50  | 12.93 
  20    |   2.123720   | 68.457195  | 3.931179 | 52.50  | 13.13 


Training complete! Best accuracy: 55.42%.

batch_size = 16
dropout = 0.0
hidden_size = 64
lr = 0.003
max_length = 25
n_epcohs = 20
n_layers = 4
n_splits = 8

input_size:  24088
output_size:  43711

4.329987438216661


=========================================================================================

 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |   5.686019   | 42.413723  | 3.275338 | 51.67  |  7.54 
   2    |   2.566756   | 64.641595  | 2.466714 | 57.29  |  7.35 
   3    |   1.144024   | 80.998654  | 2.614449 | 59.17  |  7.30 
   4    |   0.153244   | 88.973961  | 2.758064 | 59.17  |  7.16 
   5    |   0.078259   | 89.053222  | 2.794414 | 59.17  |  7.26 
   6    |   1.955650   | 72.125414  | 2.626085 | 57.29  |  7.18 
   7    |   0.992321   | 82.486622  | 3.085011 | 58.33  |  6.98 
   8    |   0.301003   | 86.635263  | 2.975448 | 59.17  |  7.17 
   9    |   0.527504   | 84.210398  | 3.244013 | 55.83  |  7.08 
  10    |   1.410121   | 76.465181  | 3.118897 | 58.33  |  7.20 
  11    |   0.331993   | 85.965517  | 3.275123 | 59.17  |  7.15 
  12    |   0.508320   | 83.448580  | 3.147060 | 58.12  |  7.27 
  13    |   0.832902   | 80.633016  | 2.849487 | 58.54  |  7.35 
  14    |   0.432463   | 84.000217  | 3.073767 | 59.17  |  7.24 
  15    |   0.382651   | 84.158532  | 3.563139 | 58.75  |  7.32 
  16    |   0.529437   | 81.873881  | 3.723119 | 57.71  |  7.31 
  17    |   0.403936   | 84.124298  | 3.552843 | 58.33  |  7.24 
  18    |   0.337393   | 84.468217  | 3.811893 | 58.75  |  7.35 
  19    |   0.311893   | 84.603525  | 3.972008 | 58.54  |  7.19 
  20    |   0.270281   | 84.633454  | 3.886294 | 58.96  |  7.27 
  21    |   0.170391   | 86.188968  | 4.169242 | 58.75  |  7.28 
  22    |   0.127603   | 87.133315  | 4.368896 | 58.75  |  7.29 
  23    |   0.122946   | 87.492571  | 4.686773 | 58.96  |  7.11 
  24    |   0.202704   | 85.569522  | 4.258096 | 58.12  |  7.26 
  25    |   0.316328   | 83.833851  | 4.181837 | 58.12  |  7.21 
  26    |   0.162343   | 86.712822  | 4.308899 | 58.96  |  7.16 
  27    |   0.062036   | 87.424277  | 4.532301 | 58.96  |  7.13 
  28    |   0.036742   | 88.816682  | 4.906375 | 59.17  |  7.15 
  29    |   0.069508   | 88.285593  | 4.832297 | 58.54  |  7.33 
  30    |   0.635877   | 80.349459  | 4.526622 | 57.29  |  7.17 
  31    |   0.468297   | 82.804021  | 4.868794 | 58.96  |  7.17 
  32    |   0.043623   | 88.630336  | 4.461898 | 59.17  |  7.18 
  33    |   0.005689   | 88.886049  | 4.527233 | 59.17  |  7.12 
  34    |   0.001774   | 89.751285  | 5.186048 | 59.17  |  7.14 
  35    |   0.001613   | 88.865011  | 5.154647 | 59.17  |  7.25 
  36    |   0.017012   | 88.122592  | 5.239184 | 58.96  |  7.21 
  37    |   2.432350   | 68.284603  | 4.209557 | 57.50  |  7.16 
  38    |   0.326140   | 85.692653  | 4.012002 | 59.17  |  7.03 
  39    |   0.006028   | 89.092142  | 4.240499 | 59.17  |  7.09 
  40    |   0.001208   | 89.041767  | 4.418566 | 59.17  |  7.24 
  41    |   0.000505   | 89.053006  | 5.158701 | 59.17  |  7.32 
  42    |   0.000949   | 89.251142  | 4.884091 | 59.17  |  7.18 
  43    |   0.004234   | 89.058378  | 5.135155 | 59.17  |  7.34 
  44    |   2.174994   | 70.733643  | 4.222369 | 56.04  |  7.25 
  45    |   0.865184   | 81.718798  | 3.810169 | 58.96  |  7.24 
  46    |   0.026721   | 89.075017  | 4.105800 | 59.17  |  7.22 
  47    |   0.003350   | 89.203672  | 4.386387 | 59.17  |  7.14 
  48    |   0.000545   | 89.833000  | 4.758155 | 59.17  |  7.10 
  49    |   0.000488   | 89.927131  | 4.859081 | 59.17  |  6.99 
  50    |   0.001296   | 88.857077  | 5.203600 | 59.17  |  7.25 


Training complete! Best accuracy: 59.17%.

batch_size = 32
dropout = 0.0
hidden_size = 128
lr = 0.003
max_length = 25
n_epcohs = 50
n_layers = 4
n_splits = 8

input_size:  24088
output_size:  43711


=========================================================================================

 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |   7.818947   | 20.644457  | 5.050583 | 29.17  |  5.69 
   2    |   3.654820   | 55.064005  | 2.902241 | 40.92  |  5.62 
   3    |   1.222336   | 78.060365  | 2.816924 | 43.30  |  5.63 
   4    |   0.030820   | 85.564576  | 2.871075 | 43.30  |  5.63 
   5    |   0.000929   | 85.995914  | 2.893517 | 43.30  |  5.61 
   6    |   0.000321   | 86.030617  | 2.911593 | 43.30  |  5.62 
   7    |   0.000118   | 86.612715  | 2.933129 | 43.30  |  5.63 
   8    |   0.000037   | 87.312899  | 2.967882 | 43.30  |  5.61 
   9    |   0.000011   | 86.248380  | 2.987403 | 43.30  |  5.68 
  10    |   0.000004   | 85.923105  | 2.994295 | 43.30  |  5.69 
  11    |   0.000001   | 86.649203  | 3.020582 | 43.30  |  5.67 
  12    |   0.000000   | 86.575818  | 3.059806 | 43.30  |  5.68 
  13    |   0.000000   | 85.816343  | 3.004143 | 43.30  |  5.69 
  14    |   0.000000   | 86.211572  | 3.022294 | 43.30  |  5.73 
  15    |   0.000000   | 86.104350  | 3.036105 | 43.30  |  5.71 
  16    |   0.000000   | 86.869531  | 3.047682 | 43.30  |  5.69 
  17    |   0.000000   | 86.503083  | 3.066070 | 43.30  |  5.71 
  18    |   0.000000   | 86.648807  | 3.077667 | 43.30  |  5.71 
  19    |   0.000000   | 87.054828  | 3.102169 | 43.30  |  5.68 
  20    |   0.000000   | 86.430471  | 3.112452 | 43.30  |  5.71 
  21    |   0.000000   | 86.795906  | 3.128914 | 43.30  |  5.71 
  22    |   0.000000   | 87.164357  | 3.140929 | 43.30  |  5.68 
  23    |   0.000000   | 86.833827  | 3.136998 | 43.30  |  5.70 
  24    |   0.000000   | 87.015082  | 3.136542 | 43.30  |  5.69 
  25    |   0.000000   | 86.393465  | 3.131506 | 43.30  |  5.72 
  26    |   0.000000   | 85.958447  | 3.117593 | 43.30  |  5.74 
  27    |   0.000000   | 86.575861  | 3.108328 | 43.30  |  5.72 
  28    |   0.000000   | 85.637086  | 3.074561 | 43.30  |  5.75 
  29    |   0.000000   | 85.314536  | 3.068666 | 43.30  |  5.72 
  30    |   0.000000   | 85.708760  | 3.056555 | 43.30  |  5.66 
  31    |   0.000000   | 86.285613  | 3.033264 | 43.30  |  5.65 
  32    |   0.000000   | 86.357060  | 3.010839 | 43.30  |  5.61 
  33    |   0.000000   | 86.031578  | 2.987699 | 43.30  |  5.60 
  34    |   0.000000   | 85.814818  | 2.954476 | 43.30  |  5.62 
  35    |   0.000000   | 86.033761  | 2.920046 | 43.30  |  5.59 
  36    |   0.000000   | 85.815907  | 2.905049 | 43.30  |  5.60 
  37    |   0.000000   | 86.685414  | 2.894928 | 43.30  |  5.59 
  38    |   0.000000   | 86.247592  | 2.881280 | 43.30  |  5.61 
  39    |   0.000000   | 87.016849  | 2.875398 | 43.30  |  5.61 
  40    |   0.000000   | 85.460138  | 2.861702 | 43.30  |  5.63 
  41    |   0.000000   | 86.140367  | 2.870513 | 43.30  |  5.63 
  42    |   0.000000   | 86.103083  | 2.858221 | 43.30  |  5.65 
  43    |   0.163150   | 84.979635  | 3.565238 | 43.23  |  5.64 
  44    |   0.003437   | 86.654564  | 3.931033 | 43.30  |  5.66 
  45    |   0.121537   | 85.211904  | 3.438089 | 43.30  |  5.67 
  46    |   0.002603   | 86.736698  | 3.320220 | 43.30  |  5.70 
  47    |   0.000112   | 86.722756  | 3.533332 | 43.30  |  5.71 
  48    |   0.000001   | 86.687042  | 3.526882 | 43.30  |  5.71 
  49    |   0.000000   | 86.540929  | 3.557493 | 43.30  |  5.71 
  50    |   0.000000   | 85.423085  | 3.653582 | 43.30  |  5.74 


Training complete! Best accuracy: 43.30%.

batch_size = 64
dropout = 0.0
hidden_size = 256
lr = 0.003
max_length = 25
n_epcohs = 50
n_layers = 4
n_splits = 8

input_size:  24088
output_size:  43711

=========================================================================================

 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |   5.933926   | 46.214563  | 3.916033 | 55.21  | 534.62
   2    |   5.053156   | 54.969315  | 3.849079 | 55.73  | 537.76
   3    |   5.329820   | 54.501208  | 4.369063 | 55.73  | 535.62
   4    |   5.632320   | 53.804101  | 4.390398 | 55.73  | 536.14
   5    |   5.707102   | 54.089485  | 4.380482 | 57.29  | 538.04
   6    |   5.732276   | 54.449195  | 4.499424 | 55.21  | 535.82
   7    |   5.716713   | 54.822625  | 4.417733 | 57.81  | 537.65
   8    |   5.679149   | 55.223333  | 4.107439 | 59.38  | 537.33
   9    |   5.634963   | 55.613693  | 4.305222 | 59.38  | 535.89
  10    |   5.591269   | 55.987920  | 4.166954 | 59.90  | 537.80
  11    |   5.544486   | 56.235068  | 4.035807 | 60.42  | 536.90
  12    |   5.510309   | 56.422777  | 4.138745 | 57.29  | 534.61
  13    |   5.485540   | 56.533390  | 4.082743 | 58.85  | 537.84
  14    |   5.457291   | 56.760545  | 4.384030 | 56.25  | 534.78
  15    |   5.437702   | 56.774728  | 4.289965 | 57.81  | 535.20
  16    |   5.414700   | 56.835586  | 3.982402 | 60.42  | 538.52
  17    |   5.401169   | 56.948902  | 4.095519 | 58.33  | 535.20
  18    |   5.387187   | 56.975969  | 3.931272 | 60.42  | 537.45
  19    |   5.379090   | 56.957177  | 4.018111 | 58.85  | 538.99
  20    |   5.370623   | 57.063174  | 4.219804 | 56.77  | 535.40
  21    |   5.355796   | 57.136032  | 4.315959 | 56.77  | 536.56
  22    |   5.349046   | 57.118814  | 4.173816 | 58.85  | 536.10
  23    |   5.350704   | 57.052652  | 4.072430 | 57.81  | 536.66
  24    |   5.341549   | 57.077294  | 4.074425 | 59.90  | 538.01
  25    |   5.341280   | 57.035996  | 4.063680 | 57.81  | 536.48
  26    |   5.340872   | 57.010278  | 3.966124 | 61.46  | 535.31
  27    |   5.330899   | 57.077008  | 4.146062 | 59.38  | 539.32
  28    |   5.335462   | 57.074137  | 4.183166 | 57.29  | 535.09
  29    |   5.331621   | 57.084119  | 4.106515 | 58.33  | 536.46
  30    |   5.333139   | 57.014912  | 4.189448 | 57.81  | 538.53
  31    |   5.336617   | 56.988487  | 3.945930 | 59.38  | 536.22
  32    |   5.331868   | 56.992014  | 3.874791 | 59.90  | 536.07
  33    |   5.328680   | 57.004347  | 3.948912 | 58.33  | 538.26
  34    |   5.327032   | 57.000519  | 4.034160 | 57.29  | 535.84
  35    |   5.329270   | 56.898319  | 4.235611 | 57.29  | 536.71
  36    |   5.332030   | 56.915407  | 4.341990 | 57.29  | 536.52
  37    |   5.334642   | 56.811778  | 4.046723 | 57.81  | 534.64
  38    |   5.329385   | 56.841400  | 4.097953 | 59.90  | 538.68
  39    |   5.337329   | 56.834137  | 4.018195 | 57.29  | 536.05
  40    |   5.331780   | 56.814272  | 4.243875 | 55.73  | 535.96
  41    |   5.326838   | 56.862383  | 3.953614 | 60.42  | 539.09
  42    |   5.322826   | 56.868046  | 3.959372 | 59.90  | 535.81
  43    |   5.324106   | 56.841112  | 4.045498 | 59.90  | 536.61
  44    |   5.324489   | 56.804739  | 4.108139 | 58.33  | 537.30
  45    |   5.325139   | 56.766861  | 4.169928 | 57.29  | 536.44
  46    |   5.317338   | 56.835760  | 4.043533 | 60.42  | 536.46
  47    |   5.314758   | 56.839838  | 3.897475 | 60.94  | 538.74
  48    |   5.312211   | 56.821525  | 4.327147 | 56.77  | 536.35
  49    |   5.304463   | 56.936623  | 3.847449 | 58.85  | 535.85
  50    |   5.299874   | 56.967614  | 3.840132 | 60.42  | 537.71


Training complete! Best accuracy: 61.46%.

batch_size = 16
dropout = 0.2
hidden_size = 64
lr = 0.0003
max_length = 20
n_epcohs = 50
n_layers = 4
n_splits = 8

input_size:  135268
output_size:  341778


=========================================================================================

batch_size = 16
dropout = 0.0
hidden_size = 64
lr = 0.0003
max_length = 20
n_epcohs = 50
n_layers = 4
n_splits = 8

input_size:  9063
output_size:  13062


=========================================================================================

 Epoch  |  Train Loss  | Train Acc  | Val Loss | Val Acc | Elapsed
--------------------------------------------------------------------------------
   1    |   8.196267   | 20.203378  | 6.828996 | 20.61  |  2.28 
   2    |   6.348423   | 33.907574  | 5.772082 | 25.45  |  2.24 
   3    |   5.328958   | 42.346933  | 5.392571 | 27.46  |  2.24 
   4    |   4.564828   | 49.099645  | 5.098601 | 30.36  |  2.21 
   5    |   3.930537   | 55.477646  | 4.660428 | 32.74  |  2.22 
   6    |   3.383774   | 62.327520  | 4.313881 | 35.42  |  2.24 
   7    |   2.908050   | 69.623668  | 4.058964 | 36.53  |  2.28 
   8    |   2.480743   | 76.822950  | 3.985410 | 36.98  |  2.25 
   9    |   2.097046   | 82.161821  | 3.887013 | 37.35  |  2.23 
  10    |   1.753530   | 84.670044  | 3.710133 | 38.47  |  2.24 
  11    |   1.444080   | 85.454457  | 3.553891 | 39.21  |  2.24 
  12    |   1.165203   | 85.593232  | 3.400765 | 39.66  |  2.26 
  13    |   0.915335   | 85.611221  | 3.426823 | 39.66  |  2.25 
  14    |   0.683997   | 85.615076  | 3.311986 | 39.81  |  2.33 
  15    |   0.492900   | 85.615076  | 3.368422 | 39.81  |  2.27 
  16    |   0.329409   | 85.615076  | 3.269029 | 39.81  |  2.30 
  17    |   0.207192   | 85.615076  | 3.202730 | 39.81  |  2.27 
  18    |   0.126325   | 85.615076  | 3.219194 | 39.81  |  2.29 
  19    |   0.074334   | 85.615076  | 3.209374 | 39.81  |  2.25 
  20    |   0.043592   | 85.615076  | 3.183645 | 39.81  |  2.34 
  21    |   0.026372   | 85.615076  | 3.230709 | 39.81  |  2.29 
  22    |   0.015843   | 85.615076  | 3.165632 | 39.81  |  2.29 
  23    |   0.009476   | 85.615076  | 3.166724 | 39.81  |  2.31 
  24    |   0.005768   | 85.615076  | 3.130527 | 39.81  |  2.26 
  25    |   0.003590   | 85.615076  | 3.151672 | 39.81  |  2.26 
  26    |   0.002205   | 85.615076  | 3.153943 | 39.81  |  2.27 
  27    |   0.001365   | 85.615076  | 3.196738 | 39.81  |  2.27 
  28    |   0.000853   | 85.615076  | 3.238304 | 39.81  |  2.25 
  29    |   0.000529   | 85.615076  | 3.154635 | 39.81  |  2.25 
  30    |   0.000335   | 85.615076  | 3.130936 | 39.81  |  2.25 
  31    |   0.000211   | 85.615076  | 3.192376 | 39.81  |  2.28 
  32    |   0.000134   | 85.615076  | 3.158317 | 39.81  |  2.26 
  33    |   0.000086   | 85.615076  | 3.178473 | 39.81  |  2.26 
  34    |   0.000056   | 85.615076  | 3.232833 | 39.81  |  2.28 
  35    |   0.000036   | 85.615076  | 3.154803 | 39.81  |  2.26 
  36    |   0.000025   | 85.615076  | 3.126508 | 39.81  |  2.27 
  37    |   0.000017   | 85.615076  | 3.210653 | 39.81  |  2.26 
  38    |   0.000012   | 85.615076  | 3.268245 | 39.81  |  2.27 
  39    |   0.000008   | 85.615076  | 3.258695 | 39.81  |  2.26 
  40    |   0.000006   | 85.615076  | 3.236533 | 39.81  |  2.30 
  41    |   0.000005   | 85.615076  | 3.238333 | 39.81  |  2.28 
  42    |   0.000003   | 85.615076  | 3.226376 | 39.81  |  2.26 
  43    |   0.000003   | 85.615076  | 3.255628 | 39.81  |  2.28 
  44    |   0.000002   | 85.615076  | 3.254656 | 39.81  |  2.25 
  45    |   0.000002   | 85.615076  | 3.262222 | 39.81  |  2.27 
  46    |   0.000001   | 85.615076  | 3.264323 | 39.81  |  2.27 
  47    |   0.000001   | 85.615076  | 3.273065 | 39.81  |  2.26 
  48    |   0.000001   | 85.615076  | 3.289328 | 39.81  |  2.26 
  49    |   0.000001   | 85.615076  | 3.301790 | 39.81  |  2.27 
  50    |   0.000001   | 85.615076  | 3.309018 | 39.81  |  2.25 


Training complete! Best accuracy: 39.81%.


batch_size = 64
dropout = 0.0
hidden_size = 256
lr = 0.0003
max_length = 25
n_epcohs = 20
n_layers = 4
n_splits = 8

input_size:  15884
output_size:  26204


=========================================================================================


batch_size = 64
dropout = 0.2
hidden_size = 256
lr = 0.0003
max_length = 25
n_epcohs = 20
n_layers = 4
n_splits = 8

input_size:  15884
output_size:  26204

test:
2.914247512817383
47.109375